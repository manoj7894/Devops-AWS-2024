# What is jenkins
-> Jenkins is an open-source automation tool written in Java. It is mainly used for
  Continuous Integration (CI) and Continuous Delivery/Deployment (CD) â€” often referred to as CI/CD.
 -> Jenkins will automate the all tasks related to building, testing, deploying and delivering the software

-> sudo cat /var/lib/jenkins/secrets/initialAdminPassword  --> Will get the password from console using this path

-> /var/lib/jenkins/  --> path of the jenkins backup 
-> /var/lib/jenkins/workspace  --> path of the jenkins build history
> vi /etc/sysconfig/jenkins [To see and change the port number of jenkins in Amazon linux server]
-> vi /etc/default/jenkins  [To see and change the port number of jenkins in Ubuntu]
-> vi /var/lib/jenkins/config.xml [if you forget the jenkins password you will go that file write false on true place]
-> We should restart the server by using sudo systemctl restart jenkins
-> vi /var/lib/jenkins/users/user.xml --> To see the user in our terminal.


# To switch between java version if we have multiple java version
-> sudo update-alternatives --config java    --> It will result as like below

There are 3 choices for the alternative java (providing /usr/bin/java).

  Selection    Path                                            Priority   Status
------------------------------------------------------------
* 0            /usr/lib/jvm/java-17-openjdk/bin/java            1711      auto mode
  1            /usr/lib/jvm/java-8-openjdk/bin/java             1081      manual mode
  2            /usr/lib/jvm/java-11-openjdk/bin/java            1111      manual mode

Press <enter> to keep the current choice[*], or type selection number:

-> sudo update-alternatives --set java /usr/lib/jvm/java-11-openjdk/bin/java


# Log path:
sudo tail -f /var/log/jenkins/jenkins.log
Common log locations:
/var/log/jenkins/jenkins.log
/var/log/syslog

sudo journalctl -u jenkins -f
----> This is the first place to look if the UI isnâ€™t loading.


# What are the different ways to trigger jenkins pipelines ?
We can trigger the pipeline in differnet ways
  - Manual trigger

  - Poll SCM: Jenkins checks your Git repo accrding cron job time. If there are new commits, it builds. If not, it skips.
 
  - Build Triggers: Jenkins will build accrding cron job time even if there are no code changes.
              
  - Webhooks: Webhook will trigger the pipeline when specific even occur in github repo.
          http://3.110.124.162:8080/github-webhook/


# How to create master and slave configurion [To overcome loads of jobs on master]
 what is master
 where we create the jobs
 Dispatch the build jobs to the slaves for execution

 what is slave
 where we execute the jobs
 It hears the request from jenkins master
 slave will build the jobs dispatched by master

No. Of Executors = This number represents at a single point in time how many different jobs can run
					in parallel


# Security
# Matrix-Based Security
-> Matrix controls users globally,  Works at JENKINS (GLOBAL) level
	Permissions are applied to the entire Jenkins instance
	Once granted, user can see ALL jobs & folders
	Cannot restrict to specific job or folders


# Project-Based Matrix Authorization
-> Works at JOB (PROJECT) level,  Project Matrix controls users per job, and 
	Permissions are set inside each job
	Same user can have different permissions per job	

# Role-Based Authorization Strategy
-> Works at GLOBAL + FOLDER + JOB level
-> Role-Based controls roles across Jenkins, folders, and jobs.
	Permissions are assigned to roles, not users
	Users are mapped to roles


# Mention some of the scripts that a DevOps engineer uses in day-to-day use cases
- Service status check - every minute checks the status of the service, If it goes down send alert notification through mail and restart the server
- monitor the disk space - every minute checks the filesystem which is consuming more and sends an email to DL
- monitor RAM (memory) space - every minute checks the CPU and MEM consumption and sends an email to DL
- Archive log files not accessed in the last 30 days
- Monitor a log file in real-time for word "failed" and send an email notification to DL


# How to secure Jenkins
- Change the default port number 
- Avoid giving Admin access to everyone
- Enable role-based authorization and give users minimum or required permissions

# How to monitor Jenkins in the UI level
- Manage Jenkins â€“ Status information â€“ System Log 


# Jenkins and tomcat server is up and running and all prerequisites installed. In Jenkins the node shows offline, how to debug node
-> we need to check node SSH connection and authentication,
-> Hostname â€“ Private ID 
-> Credential â€“ username (server default user) and password - .pem key 
-> Verify disk temp space if it is low


# Jenkin UI is down and not accessible, how to debug
- Check the server status, is it running or not if it running
-  Check Jenkins service status â€“ sudo systemctl status Jenkin
- Application log /var/lib/Jenkins/logs/health-checker.log (disk space ..)
- Default location of Jenkins log /var/log/jenkins/jenkins.log
- Jenkins service log /var/lib/Jenkins/syslog
- Real time log sudo journalctl -u jenkins -f


# How to debug job in Jenkins 
- Specific Job â€“ Build - Console output


# How to get admin access to Jenkins
- Already if a person is admin can give admin access to other users,
Manage Jenkins â€“ Manage users â€“ Assign permissions or add to admin role
(Role based Authorization strategy)
Or 
Manage Jenkins â€“ Security - Under Authorization - Enable Matrix-based security - Add your username â€“ Checkmark Overall (Administer)
- If you have not set yourself as admin,
1. Manage Jenkins â€“ Under Security - Security - Security Realm (Jenkinsâ€™ own user database) â€“ 
2. Manage Jenkins â€“ Security â€“ Authorization â€“ Matrix-based Security â€“ Add user â€“ checkmark overall for the user


# If you loose Jenkin level access completely, how do you access 
Jenkin server â€“ 
sudo systemctl stop Jenkins
sudo vi /var/lib/Jenkins/config.xml
<useSecurity>false</useSecurity>
Sudo systemctl start Jenkin
Reload Jenkinâ€™s URL allow you to login to Jenkins without login details


# How to change the port number of jenkin
- We have to stop the Jenkin service 
sudo systemctl stop Jenkins
-	We have to change the port number in the Jenkins default config 
sudo vi /etc/default/jenkins 
HTTP_PORT=8080 
Change to: HTTP_PORT=8090
-	We have to add override file (override file is used to change or extend service settings safely without touching the original service file)

Create a directory to place override file 
sudo mkdir -p /etc/systemd/system/jenkins.service.d 

Create and paste below service details
sudo vi /etc/systemd/system/jenkins.service.d/override.conf 
[Service] 
Environment="JENKINS_PORT=8090"

-	Run the below systemd command that tells the system to re-read service configuration files.
sudo systemctl daemon-reload

-	Start Jenkin service
sudo systemctl start jenkins


# what is the difference between freestyle job and pipeline job in jenkins
Freestyle Job
-> A Freestyle Job is the basic, GUI-based job type in Jenkins.
-> In Freestyle job we can able to select one agent only

# Pipeline Job
-> A Pipeline Job uses code to define the CI/CD workflow written as a Jenkinsfile.
-> In Pipeline job we can able to select multiple agent only, For each job run on paricular agent.


# What is Downtime in Jenkins?
-> Downtime in Jenkins means the period when the Jenkins server is not available to users or cannot run jobs/builds.

When does Jenkins downtime happen
-> Jenkins service is stopped or restarted
          sudo systemctl restart jenkins
-> Jenkins is upgrading
          Jenkins version upgrade
          Plugin upgrades that require restart
-> Server or OS reboot
          EC2 reboot
          VM restart
          Power failure
-> Port or configuration change
          Changing Jenkins port
          Modifying /etc/default/jenkins
          JVM options change
-> High load / crash
          Out of memory (OOM)
          Disk full
          Too many jobs running
-> Network issues
          Firewall block
          DNS issue
          Load balancer problem


# Types of Downtime
-> Planned Downtime
          Scheduled maintenance
          Jenkins upgrade
          Plugin updates
          Expected and communicated

-> Unplanned Downtime
          Server crash
          Disk full
          JVM crash
          Hardware/network failure
          Unexpected


# What if jenkins master server crash
-> Jenkins UI becomes unavailable
          You cannot access Jenkins dashboard
          Job triggers stop
-> Running builds are affected
          Builds running on the master â†’ âŒ fail immediately
          Builds running on agents:
          Usually stop because controller coordination is lost
          Some tools may continue briefly but results are not recorded
-> No new builds can start
          Webhooks, cron jobs, manual triggers â†’ âŒ wonâ€™t work


Immediate Actions
1. Check server status
          systemctl status jenkins
2. Check logs
          journalctl -u jenkins -n 100
          /var/log/jenkins/jenkins.log

Common causes:
Out of memory (OOM)
Disk full
Port conflict
Plugin crash

3. Restart Jenkins
          systemctl restart jenkins

4. Verify Jenkins Home
          /var/lib/jenkins



How Production Teams Prevent Jenkins Master Crash Impact
-> Similar we should maintain one server resemble of master
-> Use Jenkins Agents (Best Practice)
          Never run builds on master
          Master only schedules jobs

-> Regular Backups
          /var/lib/jenkins

-> Use Persistent Storage
-> Monitoring & Alerts
-> High Availability (Advanced)


Interview-Ready Answer

If Jenkins master crashes, the Jenkins UI becomes unavailable, all scheduling stops, running jobs fail, and no new builds can start. Recovery depends on restoring the Jenkins controller and Jenkins home from backup. Best practice is to use agents, backups, and persistent storage to minimize impact.


# What Happens If a Jenkins Slave / Agent Node Is Down?
-> Jenkins master stays UP
-> Jobs assigned to that slave fail or wait
          Running jobs on that agent â†’ âŒ fail
          Queued jobs waiting for that agent â†’ â¸ï¸ wait until node comes back
-> Other agents continue working


Immediate Actions to Recover Agent
1. Check agent status
          Manage Jenkins â†’ Nodes â†’ <agent-name>
2. Check agent service (on slave)
          systemctl status ssh

Common Reasons for Agent Down
          Server reboot or crash
          Network issue
          Disk full
          Java not running
          SSH key issue
          Agent JVM killed (OOM)

Best Practices to Minimize Impact
-> Multiple agents with same labels
          Jobs can run on any available agent
-> Auto-reconnect agents
          Enable Keep agent online
          Use inbound agents

Interview-Ready Answer

If a Jenkins slave node goes down, the Jenkins master remains available. Jobs running on that agent fail, queued jobs wait, and other agents continue executing builds. Once the agent is restored or replaced, jobs resume normally.



Plugins:

Preinstalled plugins,
- Git plugin (This plugin integrates Git with Jenkin)
- Github plugin (This plugin integrates Github to Jenkin) 
- SSH Credentials Plugin (Allows storage of SSH credentials in Jenkin)
- Build Timeout (This plugin allows you to automatically terminate a build if it's taking too long)


Plugins help in increasing the capabilities of Jenkins.

1. Monitoring --> To Monitor Jenkins Server
2. Docker --> To integrate Docker with Jenkins
3. K8S --> To integrate K8S with Jenkins
4. Blueocean --> To visualize Jenkin Jobs
5. Periodic backup --> To take Periodically backup of Jenkins serve
6. Parameterized Plugins --> To pass inputs to Jenkins jobs
7. Maven Plugin --> To work on Maven build tool in Jenkins
8. Artifactory --> To integrate Jfrog with jenkins
8. CVS - Source Code Management Plugins
9. Pipeline: Stage View ----> To see the stages in the job
10. Role-based Authorization Strategy ----> To limit the access to the user on the Project level

# how many slaves we need when we have 100 microservices?
In real production, we use agents (slaves) are according sized by workload, not by service count.

# Master is in good high capability then why we need slave?
Even if Jenkins master has high capacity, we use slaves (agents) to ensure stability, scalability, security, and fault isolation. The master is designed for orchestration, while agents are meant to execute resource-intensive jobs.

# Microservices Approach (Real Production Way)
User Service
Product Service
Cart Service
Order Service
Payment Service
Notification Service

Each service:
Has its own code
Has its own database
Is deployed independently

# What is pipeline
Parallel pipelines are used when independent tasks can run simultaneously to reduce total build and deployment time
Parallel pipelines reduce total time by running tasks inside a stage concurrently, even though stages themselves execute sequentially.


1) Can you explain the CICD process in your current project ? or Can you talk about any CICD process that you have implemented ?
-> Sure! In my current project, Iâ€™ve implemented a full CI/CD pipeline using Jenkins Pipelines to automate the build, test, security scan, manual approval, and deployment workflows from source code to production.
-> We use GitHub as our version control system and Amazon EKS (Elastic Kubernetes Service) as our deployment platform.  
-> The pipeline consists of multiple stages, each focused on a specific area of quality, security, or deployment. Here's a breakdown of each stage:

Stage-1:- (Checkout) It will clone the source code from github repository
Stage-2:- (Install Package Dependencies) Install the required dependencies needed for the application to run
Stage-3:- (Unit Test) Runs unit tests to ensure application logic works as expected
Stage-4:- (Trivy file system scan) Trivy it will run the mulitple stages. First one it will scan the files and folders means it will scan the manifeast files, yaml file, terraform fiels and other files to detect the vulnerabilities. AS well as if we keep any password and tokens in that files and folder it will let us know to make sure dont expose
Stage-5:- (OWASP Scan) It will check the dependencies in our dependencies in files like package.json
Stage-6:- (Code Scan) Scan the source code using SonarQube to identify bugs, vulnerabilities, and code smells.
Stage-7:- (Build) Build the docker image based on docker file
Stage-8:- (Image Scan) We are using trivy to scan the docker image to decect the vulnerabilities
Stage-9:- (Push) Push the docker image into dockerhub where we are Storing our docker images
Stage-10:- (Update the manifeastfiles)  Update the Docker image reference in the Kubernetes manifest file stored in the repository.
Stage-11:- (Continuous delivery) A manual approval step is configured before deployment 
          Enables the DevOps or QA team to verify and approve the changes
Stage-12:- (Deploy to EKS) Deploy the updated manifest file to the Amazon EKS (Elastic Kubernetes Service) cluster kubectl apply, ensuring automated deployment to the Kubernetes platform.


# Bitbucket Pipeline
# Can you explain the CICD process in your current project ? or Can you talk about any CICD process that you have implemented ?
-> Sure! In my current project, Iâ€™ve implemented a full CI/CD pipeline using Bitbucket Pipelines to automate the build, test, security scan, manual approval, and deployment workflows from source code to production.
We use Bitbucket as our version control system and Amazon EKS (Elastic Kubernetes Service) as our deployment platform.  
The pipeline consists of multiple stages, each focused on a specific area of quality, security, or deployment. Here's a breakdown of each stage:

Stage-1:- As soon as code is pushed to the release/production branch, the pipeline is triggered and It will install required dependencies and builds the Node.js application using npm
Stage-2:- (Unit Test) Runs unit tests to ensure application logic works as expected
Stage-3:- (Trivy file system scan) Trivy it will run the mulitple stages. First one it will scan the files and folders means it will scan the manifeast files, yaml file, terraform fiels and other files to detect the vulnerabilities. AS well as if we keep any password and tokens in that files and folder it will let us know to make sure dont expose
Stage-4:- (Code Scan) Scan the source code using SonarQube to identify bugs, vulnerabilities, and code smells.
Stage-5:- (Build) Build the docker image based on docker file
Stage-6:- (Image Scan) We are using trivy to scan the docker image to decect the vulnerabilities
Stage-7:- (Continuous delivery) A manual approval step is configured before deployment 
          Enables the DevOps or QA team to verify and approve the changes.
Stage-8:- (Push) The approved Docker image is pushed to Amazon ECR

AWS App Runner is configured to automatically pull and deploy the latest image from ECR
This ensures zero-downtime, scalable deployment of the backend service.


# Real-Time Challenges Faced While Implementing the CI/CD Pipeline
-> While building and maintaining our Bitbucket CI/CD pipeline for AWS App Runner deployments, I encountered several real-world challenges that helped me enhance the overall pipeline design and reliability.

1. Handling Environment-Specific Configurations
Managing environment variables across multiple environments such as Dev, UAT, Staging, and Production was a key challenge.
To address this, I used Bitbucket Deployment Variables to maintain separate configurations for each environment, ensuring smooth and isolated deployments.

2. Secure Management of Secrets
Storing and managing sensitive information like AWS access keys and secret keys securely was crucial.
I used Bitbucket Secure Environment Variables to protect these secrets and prevent accidental exposure in logs or code.

3. Trivy Scan Failures Due to Limited Disk Space
Initially, Trivy security scans failed because of limited disk space during the pipeline execution.
I optimized the build cache and excluded unnecessary directories from the scan to resolve this issue.

4. Memory Issues During Docker Builds
We encountered memory constraints while building large Docker images in the CI environment.
The issue was mitigated by splitting the Docker build into multiple stages and using self-hosted runners with higher resource capacity.



# I have faced some Challenges while i am deploying the backend application in Apprunner services
-> At Starting stage App Runner used to failed to deploy due to Dockerfile or image issues from ECR 
   for that we should run the docker image build in our local before push into ECR.

-> Health check used to become failed due to passing incorrect port number and variables.
    we should make sure weather we are passing correct port number or not before deploy the application into apprunner

-> During high load, App Runner services struggled to scale efficiently then Deployment will fail.
-> we have to go concurrency settings and increased CPU/memory allocation.


I have faced some Challenges while i am deploying the frontend application in amplify services
Mostly Initial deployments failed due to incorrect build configurations and missing environment variables in the amplify.yml file.
we should make sure before deploy

Amplify sometimes used cached builds, which led to outdated files being served.
Cleared build cache manually in Amplify Console and adjusted cache settings in amplify.yml

Frontend failed to connect to backend due to misconfigured endpoints.
we should make sure to passing backend URL before deploy

Taking time to linking custom domains and automatic SSL provisioning 
we should ensured DNS records were correctly pointed or not





3) How to backup Jenkins ?
-> Taking a backup of Jenkins is critical to protect our build configurations, job data, plugins, and credentials.
-> but it's very easy to back it up manually or with scripts.

-> We mainly need to back up the Jenkins home directory â€” it contains everything:
    /var/lib/jenkins/   â† Default Jenkins home

| Folder/File       | What it contains                             |
| ----------------- | -------------------------------------------- |
| `jobs/`           | All job configs, history, workspace          |
| `config.xml`      | Global Jenkins settings                      |
| `plugins/`        | Installed plugins                            |
| `users/`          | User accounts                                |
| `secrets/`        | Credentials and encrypted secrets            |
| `credentials.xml` | Secure credential storage                    |
| `nodes/`          | Agent (node) configuration                   |
| `workspace/`      | Job working directories (can skip if needed) |

# Manually Backup
-> First we have to stop the server before we are going to take backup
sudo systemctl stop jenkins    # Stop Jenkins to ensure consistency
sudo cp -r /var/lib/jenkins /var/backups/jenkins-backup-$(date +%F)
sudo systemctl start jenkins   # Restart Jenkins after backup

# Automated Backup with a Script
#!/bin/bash

BACKUP_DIR="/var/backups/jenkins"
JENKINS_HOME="/var/lib/jenkins"
DATE=$(date +%F)
mkdir -p $BACKUP_DIR

tar -czf $BACKUP_DIR/jenkins-backup-$DATE.tar.gz $JENKINS_HOME

-> crontab -e
-> 0 2 * * * /path/to/jenkins_backup.sh

# How to restore the Jenkins Backup
-> sudo systemctl stop jenkins
-> sudo rm -rf /var/lib/jenkins/*
-> sudo cp -r /path/to/jenkins-backup/* /var/lib/jenkins/
-> sudo chown -R jenkins:jenkins /var/lib/jenkins
-> sudo systemctl start jenkins


4) How do you store/secure/handle secrets in Jenkins
 There are multiple ways to achieve this, Let me give you a brief explanation of all the posible options.
   - Credentials Plugin: Jenkins provides a credentials plugin that can be used to store secrets such as passwords, API keys, and certificates. The secrets are encrypted and stored securely within Jenkins, and can be easily retrieved in build scripts or used in other plugins.
   
   - Environment Variables: Secrets can be stored as environment variables in Jenkins and referenced in build scripts. However, this method is less secure because environment variables are visible in the build logs.
   
   - Hashicorp Vault: Jenkins can be integrated with Hashicorp Vault, which is a secure secrets management tool. Vault can be used to store and manage sensitive information, and Jenkins can retrieve the secrets as needed for builds.
   
   - Third-party Secret Management Tools: Jenkins can also be integrated with third-party secret management tools such as AWS Secrets Manager, Google Cloud Key Management Service, and Azure Key Vault.

5) What is latest version of Jenkins or which version of Jenkins are you using 

6) What is shared modules in Jenkins ?
-> Shared Modules usually refer to Shared Libraries â€” a way to reuse pipeline code across multiple Jenkins pipelines or jobs.
âœ… What it does:
-> Keeps your pipeline logic clean and DRY (Donâ€™t Repeat Yourself)
-> Lets you write common code (functions, stages, tools) once and reuse it across jobs
-> Supports versioning, branches, and testing

7) can you use Jenkins to build applications with multiple programming languages using different agents in different stages ?
-> Absolutely, YES â€” Jenkins is perfectly suited to build applications with multiple programming languages using different agents in different stages of a pipeline.


8) How to setup auto-scaling group for Jenkins in AWS 

9) How to add a new worker node in Jenkins ?
Log into the Jenkins master and navigate to Manage Jenkins > Manage Nodes > New Node. Enter a name for the new node and select Permanent Agent. Configure SSH and click on Launch.

10) How to add a new plugin in Jenkins ?
Using the CLI, java -jar jenkins-cli.jar install-plugin <PLUGIN_NAME>

Using the UI,
Download the .hpi or .jpi file for the custom plugin from the source where it's available. This might be a direct download link from a website, a GitHub repository, or another distribution method.
Upload the Plugin:
Click on the "Manage Jenkins" link in the left-side menu.
Click on the "Manage Plugins" link.

11) What is JNLP and why is it used in Jenkins ?
JNLP stands for Java Network Launch Protocol.
Itâ€™s a protocol used to launch Java applications over a network.
In Jenkins, JNLP is used to connect agents (slaves) to the Jenkins controller (master).


12) What are some of the common plugins that you use in Jenkins
-> JDK Plugins
-> Docker and Docker pipeline plugins
-> Sonar-Scanner
-> Dependency-Check
-> Email-extension plugin
-> Kuberntes Plugins

13) Why we are using declarative pipeline script
-> Scripted pipeline does not have more flexibility compare to declarative pipeline.
-> Using Declarative pipeline we can easily collaborate with people and everyone can easily understand

| Feature     | Declarative Pipeline         | Scripted Pipeline                        |
| ----------- | ---------------------------- | ---------------------------------------- |
| Syntax      | Easy, structured             | Flexible, but more complex (Groovy code) |
| Readability | Beginner-friendly            | Requires Groovy knowledge                |
| Use case    | Most CI/CD use cases         | Complex logic, dynamic stages            |
| Example     | `pipeline { stages { ... }}` | `node { stage('Build') { ... } }`        |


14) How do you handle the issues if your worknode is down and not responce
Go to: Jenkins Dashboard â†’ Manage Jenkins â†’ Manage Nodes
Look for red ðŸ”´ status or "offline" message
Hover or click the agent to see offline reason

| Problem                    | Fix                                                       |
| -------------------------- | --------------------------------------------------------- |
| âŒ Node is shut down        | Restart the EC2, VM, or container                         |
| ðŸ”Œ Lost network connection | Check network/firewall rules                              |
| ðŸ” SSH/JNLP auth failure   | Reverify credentials, keys, or secrets                    |
| ðŸ˜ Disk full               | Clear space on the agent machine                          |
| ðŸ›‘ Agent service stopped   | Restart the Jenkins agent process (`java -jar agent.jar`) |


15) There is developer who has commit the change in git repo how deos your jenkins notify ?
-> Using webhooks

16) What type of agnet are you using in jenkins
-> Docker agent because it is light weight and dont need do lots of installization

# What is the difference between Quality Profile and Quality Gate
-> A Quality Profile in SonarQube defines the set of rules for analyzing code quality, 
-> while a Quality Gate defines the criteria that a project must meet to pass a quality check.

17) What are some common challenges in CI/CD, and how have you resolved them?

Common challenges include:

- Ensuring consistent environments across development, testing, and production.
- Managing dependencies and versioning.
- Handling failed builds or deployments.

I have resolved these challenges by:
- Using containerization (e.g., Docker) to ensure consistent environments.
- Implementing dependency management tools (e.g., Maven, npm).
- Configuring automated rollbacks and notifications for failed builds or deployments.

â€œWhile Implemeted CI/CD with Jenkins, we faced real-world challenges like slow pipelines, secret leakage, and flaky tests. 
We tackled these by optimizing build time using parallel stages, securing secrets using AWS Secrets Manager, and improving code quality with feedback loops in SonarQube. 
Each issue helped us fine-tune the pipeline for better performance, security, and reliability.â€




# Upstream Job (Triggering Job)
An Upstream Job is a job that runs first and triggers another job when it completes.

# Downstream Job (Triggered Job)
A Downstream Job is a job that runs after another job is completed.


Pipeline Syntax:
Two types of syntax are used for defining your JenkinsFile.

DECLARATIVE PIPELINE: 					
1. New way of writing with simple groovy declaratives
2. All code is defined inside pipeline block

SCRIPTED PIPELINE:
1. Old way / Traditional way of pipeline code with scripted groovy syntax
2. Defined within a node block


# Multi branch Pipeline
A Multi-Branch Pipeline is a Jenkins job type that can automatically handle multiple branches of your source code repository without creating a separate job for each branch.
Instead of manually creating a pipeline per branch, Jenkins automatically:

Scans your repository for branches
Creates temporary pipeline jobs for each branch it finds
Runs builds according to a Jenkinsfile in that branch

| Branch        | New commit? | Build triggered? |
| ------------- | ----------- | ---------------- |
| main          | yes         | âœ… Yes          |
| develop       | no          | âŒ No           |
| feature/login | no          | âŒ No           |


Note:
Multi-branch pipelines are event-driven by default: triggered on new commits or PRs
Only the branch with new commits is built by default
Jenkins does not blindly build all branches every time


Using Sonarqube it will check code quality check and code coverage
Code quality Check:- It find the is there any bugs, code smells and vulnerability in our source code written by developer
Code Coverage:- It will find how much percentage of code is tested by running unit test cases

Trivy it can perform two things to scan
-> Files System Scan
-> Image Scan

What is File system scan
It will scan our files and folders to detect the dependencies vulnerability and sensitive data. it will scan our dependencies we call libraries which is required for our application to run, it will check dependencies are safe to use or not if it not safe it will tell us what version is safe. Secrets If keep any secrets in our source code it will let us know don't expose any secrets in our code.

What is Image scan
It will scan our docker images to detect the vulnerability and It will check whether image build correctly or not.


# 3 tier project link
https://youtu.be/lh5f69t7L9A?si=se72MTCtaG1Iw1UI
