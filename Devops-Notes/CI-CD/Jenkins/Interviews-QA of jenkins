1) Can you explain the CICD process in your current project ? or Can you talk about any CICD process that you have implemented ?
-> Sure! In my current project, Iâ€™ve implemented a full CI/CD pipeline using Jenkins Pipelines to automate the build, test, security scan, manual approval, and deployment workflows from source code to production.
-> We use GitHub as our version control system and Amazon EKS (Elastic Kubernetes Service) as our deployment platform.  
-> The pipeline consists of multiple stages, each focused on a specific area of quality, security, or deployment. Here's a breakdown of each stage:

Stage-1:- (Checkout) It will clone the source code from github repository
Stage-2:- (Install Package Dependencies) Install the required dependencies needed for the application to run
Stage-3:- (Unit Test) Runs unit tests to ensure application logic works as expected
Stage-4:- (Trivy file system scan) Trivy it will run the mulitple stages. First one it will scan the files and folders means it will scan the manifeast files, yaml file, terraform fiels and other files to detect the vulnerabilities. AS well as if we keep any password and tokens in that files and folder it will let us know to make sure dont expose
Stage-5:- (OWASP Scan) It will check the dependencies in our dependencies in files like package.json
Stage-6:- (Code Scan) Scan the source code using SonarQube to identify bugs, vulnerabilities, and code smells.
Stage-7:- (Build) Build the docker image based on docker file
Stage-8:- (Image Scan) We are using trivy to scan the docker image to decect the vulnerabilities
Stage-9:- (Push) Push the docker image into dockerhub where we are Storing our docker images
Stage-10:- (Update the manifeastfiles)  Update the Docker image reference in the Kubernetes manifest file stored in the repository.
Stage-11:- (Continuous delivery) A manual approval step is configured before deployment 
          Enables the DevOps or QA team to verify and approve the changes
Stage-12:- (Deploy to EKS) Deploy the updated manifest file to the Amazon EKS (Elastic Kubernetes Service) cluster kubectl apply, ensuring automated deployment to the Kubernetes platform.


# Bitbucket Pipeline
# Can you explain the CICD process in your current project ? or Can you talk about any CICD process that you have implemented ?
-> Sure! In my current project, Iâ€™ve implemented a full CI/CD pipeline using Bitbucket Pipelines to automate the build, test, security scan, manual approval, and deployment workflows from source code to production.
We use Bitbucket as our version control system and Amazon EKS (Elastic Kubernetes Service) as our deployment platform.  
The pipeline consists of multiple stages, each focused on a specific area of quality, security, or deployment. Here's a breakdown of each stage:

Stage-1:- As soon as code is pushed to the release/production branch, the pipeline is triggered and It will install required dependencies and builds the Node.js application using npm
Stage-2:- (Unit Test) Runs unit tests to ensure application logic works as expected
Stage-3:- (Trivy file system scan) Trivy it will run the mulitple stages. First one it will scan the files and folders means it will scan the manifeast files, yaml file, terraform fiels and other files to detect the vulnerabilities. AS well as if we keep any password and tokens in that files and folder it will let us know to make sure dont expose
Stage-4:- (Code Scan) Scan the source code using SonarQube to identify bugs, vulnerabilities, and code smells.
Stage-5:- (Build) Build the docker image based on docker file
Stage-6:- (Image Scan) We are using trivy to scan the docker image to decect the vulnerabilities
Stage-7:- (Continuous delivery) A manual approval step is configured before deployment 
          Enables the DevOps or QA team to verify and approve the changes.
Stage-8:- (Push) The approved Docker image is pushed to Amazon ECR

AWS App Runner is configured to automatically pull and deploy the latest image from ECR
This ensures zero-downtime, scalable deployment of the backend service.


# Real-Time Challenges Faced While Implementing the CI/CD Pipeline
-> While building and maintaining our Bitbucket CI/CD pipeline for AWS App Runner deployments, I encountered several real-world challenges that helped me enhance the overall pipeline design and reliability.

1. Handling Environment-Specific Configurations
Managing environment variables across multiple environments such as Dev, UAT, Staging, and Production was a key challenge.
To address this, I used Bitbucket Deployment Variables to maintain separate configurations for each environment, ensuring smooth and isolated deployments.

2. Secure Management of Secrets
Storing and managing sensitive information like AWS access keys and secret keys securely was crucial.
I used Bitbucket Secure Environment Variables to protect these secrets and prevent accidental exposure in logs or code.

3. Trivy Scan Failures Due to Limited Disk Space
Initially, Trivy security scans failed because of limited disk space during the pipeline execution.
I optimized the build cache and excluded unnecessary directories from the scan to resolve this issue.

4. Memory Issues During Docker Builds
We encountered memory constraints while building large Docker images in the CI environment.
The issue was mitigated by splitting the Docker build into multiple stages and using self-hosted runners with higher resource capacity.



# I have faced some Challenges while i am deploying the backend application in Apprunner services
-> At Starting stage App Runner used to failed to deploy due to Dockerfile or image issues from ECR 
   for that we should run the docker image build in our local before push into ECR.

-> Health check used to become failed due to passing incorrect port number and variables.
    we should make sure weather we are passing correct port number or not before deploy the application into apprunner

-> During high load, App Runner services struggled to scale efficiently then Deployment will fail.
-> we have to go concurrency settings and increased CPU/memory allocation.


I have faced some Challenges while i am deploying the frontend application in amplify services
Mostly Initial deployments failed due to incorrect build configurations and missing environment variables in the amplify.yml file.
we should make sure before deploy

Amplify sometimes used cached builds, which led to outdated files being served.
Cleared build cache manually in Amplify Console and adjusted cache settings in amplify.yml

Frontend failed to connect to backend due to misconfigured endpoints.
we should make sure to passing backend URL before deploy

Taking time to linking custom domains and automatic SSL provisioning 
we should ensured DNS records were correctly pointed or not


2) What are the different ways to trigger jenkins pipelines ?
We can trigger the pipeline in differnet ways

  - Poll SCM: Jenkins checks your Git repo accrding cron job time. If there are new commits, it builds. If not, it skips.

              
  - Build Triggers: Jenkins will build accrding cron job time even if there are no code changes.
              
  - Webhooks: Webhook will trigger the pipeline when specific even occur in github repo.


3) How to backup Jenkins ?
-> Taking a backup of Jenkins is critical to protect our build configurations, job data, plugins, and credentials.
-> but it's very easy to back it up manually or with scripts.

-> We mainly need to back up the Jenkins home directory â€” it contains everything:
    /var/lib/jenkins/   â† Default Jenkins home

| Folder/File       | What it contains                             |
| ----------------- | -------------------------------------------- |
| `jobs/`           | All job configs, history, workspace          |
| `config.xml`      | Global Jenkins settings                      |
| `plugins/`        | Installed plugins                            |
| `users/`          | User accounts                                |
| `secrets/`        | Credentials and encrypted secrets            |
| `credentials.xml` | Secure credential storage                    |
| `nodes/`          | Agent (node) configuration                   |
| `workspace/`      | Job working directories (can skip if needed) |

# Manually Backup
-> First we have to stop the server before we are going to take backup
sudo systemctl stop jenkins    # Stop Jenkins to ensure consistency
sudo cp -r /var/lib/jenkins /var/backups/jenkins-backup-$(date +%F)
sudo systemctl start jenkins   # Restart Jenkins after backup

# Automated Backup with a Script
#!/bin/bash

BACKUP_DIR="/var/backups/jenkins"
JENKINS_HOME="/var/lib/jenkins"
DATE=$(date +%F)
mkdir -p $BACKUP_DIR

tar -czf $BACKUP_DIR/jenkins-backup-$DATE.tar.gz $JENKINS_HOME

-> crontab -e
-> 0 2 * * * /path/to/jenkins_backup.sh

# How to restore the Jenkins Backup
-> sudo systemctl stop jenkins
-> sudo rm -rf /var/lib/jenkins/*
-> sudo cp -r /path/to/jenkins-backup/* /var/lib/jenkins/
-> sudo chown -R jenkins:jenkins /var/lib/jenkins
-> sudo systemctl start jenkins


4) How do you store/secure/handle secrets in Jenkins
 There are multiple ways to achieve this, Let me give you a brief explanation of all the posible options.
   - Credentials Plugin: Jenkins provides a credentials plugin that can be used to store secrets such as passwords, API keys, and certificates. The secrets are encrypted and stored securely within Jenkins, and can be easily retrieved in build scripts or used in other plugins.
   
   - Environment Variables: Secrets can be stored as environment variables in Jenkins and referenced in build scripts. However, this method is less secure because environment variables are visible in the build logs.
   
   - Hashicorp Vault: Jenkins can be integrated with Hashicorp Vault, which is a secure secrets management tool. Vault can be used to store and manage sensitive information, and Jenkins can retrieve the secrets as needed for builds.
   
   - Third-party Secret Management Tools: Jenkins can also be integrated with third-party secret management tools such as AWS Secrets Manager, Google Cloud Key Management Service, and Azure Key Vault.

5) What is latest version of Jenkins or which version of Jenkins are you using 

6) What is shared modules in Jenkins ?
-> Shared Modules usually refer to Shared Libraries â€” a way to reuse pipeline code across multiple Jenkins pipelines or jobs.
âœ… What it does:
-> Keeps your pipeline logic clean and DRY (Donâ€™t Repeat Yourself)
-> Lets you write common code (functions, stages, tools) once and reuse it across jobs
-> Supports versioning, branches, and testing

7) can you use Jenkins to build applications with multiple programming languages using different agents in different stages ?
-> Absolutely, YES â€” Jenkins is perfectly suited to build applications with multiple programming languages using different agents in different stages of a pipeline.


8) How to setup auto-scaling group for Jenkins in AWS 

9) How to add a new worker node in Jenkins ?
Log into the Jenkins master and navigate to Manage Jenkins > Manage Nodes > New Node. Enter a name for the new node and select Permanent Agent. Configure SSH and click on Launch.

10) How to add a new plugin in Jenkins ?
Using the CLI, java -jar jenkins-cli.jar install-plugin <PLUGIN_NAME>

Using the UI,
Download the .hpi or .jpi file for the custom plugin from the source where it's available. This might be a direct download link from a website, a GitHub repository, or another distribution method.
Upload the Plugin:
Click on the "Manage Jenkins" link in the left-side menu.
Click on the "Manage Plugins" link.

11) What is JNLP and why is it used in Jenkins ?
JNLP stands for Java Network Launch Protocol.
Itâ€™s a protocol used to launch Java applications over a network.
In Jenkins, JNLP is used to connect agents (slaves) to the Jenkins controller (master).


12) What are some of the common plugins that you use in Jenkins
-> JDK Plugins
-> Docker and Docker pipeline plugins
-> Sonar-Scanner
-> Dependency-Check
-> Email-extension plugin
-> Kuberntes Plugins

13) Why we are using declarative pipeline script
-> Scripted pipeline does not have more flexibility compare to declarative pipeline.
-> Using Declarative pipeline we can easily collaborate with people and everyone can easily understand

| Feature     | Declarative Pipeline         | Scripted Pipeline                        |
| ----------- | ---------------------------- | ---------------------------------------- |
| Syntax      | Easy, structured             | Flexible, but more complex (Groovy code) |
| Readability | Beginner-friendly            | Requires Groovy knowledge                |
| Use case    | Most CI/CD use cases         | Complex logic, dynamic stages            |
| Example     | `pipeline { stages { ... }}` | `node { stage('Build') { ... } }`        |


14) How do you handle the issues if your worknode is down and not responce
Go to: Jenkins Dashboard â†’ Manage Jenkins â†’ Manage Nodes
Look for red ðŸ”´ status or "offline" message
Hover or click the agent to see offline reason

| Problem                    | Fix                                                       |
| -------------------------- | --------------------------------------------------------- |
| âŒ Node is shut down        | Restart the EC2, VM, or container                         |
| ðŸ”Œ Lost network connection | Check network/firewall rules                              |
| ðŸ” SSH/JNLP auth failure   | Reverify credentials, keys, or secrets                    |
| ðŸ˜ Disk full               | Clear space on the agent machine                          |
| ðŸ›‘ Agent service stopped   | Restart the Jenkins agent process (`java -jar agent.jar`) |


15) There is developer who has commit the change in git repo how deos your jenkins notify ?
-> Using webhooks

16) What type of agnet are you using in jenkins
-> Docker agent because it is light weight and dont need do lots of installization

# What is the difference between Quality Profile and Quality Gate
-> A Quality Profile in SonarQube defines the set of rules for analyzing code quality, 
-> while a Quality Gate defines the criteria that a project must meet to pass a quality check.

17) What are some common challenges in CI/CD, and how have you resolved them?

Common challenges include:

- Ensuring consistent environments across development, testing, and production.
- Managing dependencies and versioning.
- Handling failed builds or deployments.

I have resolved these challenges by:
- Using containerization (e.g., Docker) to ensure consistent environments.
- Implementing dependency management tools (e.g., Maven, npm).
- Configuring automated rollbacks and notifications for failed builds or deployments.

â€œWhile Implemeted CI/CD with Jenkins, we faced real-world challenges like slow pipelines, secret leakage, and flaky tests. 
We tackled these by optimizing build time using parallel stages, securing secrets using AWS Secrets Manager, and improving code quality with feedback loops in SonarQube. 
Each issue helped us fine-tune the pipeline for better performance, security, and reliability.â€




Using Sonarqube it will check code quality check and code coverage
Code quality Check:- It find the is there any bugs, code smells and vulnerability in our source code written by developer
Code Coverage:- It will find how much percentage of code is tested by running unit test cases

Trivy it can perform two things to scan
-> Files System Scan
-> Image Scan

What is File system scan
It will scan our files and folders to detect the dependencies vulnerability and sensitive data. it will scan our dependencies we call libraries which is required for our application to run, it will check dependencies are safe to use or not if it not safe it will tell us what version is safe. Secrets If keep any secrets in our source code it will let us know don't expose any secrets in our code.

What is Image scan
It will scan our docker images to detect the vulnerability and It will check whether image build correctly or not.


# 3 tier project link
https://youtu.be/lh5f69t7L9A?si=se72MTCtaG1Iw1UI
