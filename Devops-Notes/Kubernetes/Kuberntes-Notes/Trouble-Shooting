ImagePullbackoff Error:
  ImagePull:- You will get this error when you keep the invid image name or non-existing image in your manifeast files you will get this kind of imagepull of error.
  Backoff error:- Due to any network issues kubectl are not able take the image at that you will get this Backoff error
  You will get both error at a time but we have explore differently each one.

https://youtu.be/vGab4v3RWEw?si=9F5sTjDGBwimK8bI



CrashLoopBackoff Error:
  It means our pod is crashing again and again
  Based on below reasons we will get this error
  -> Memory limit low [You keep some memory limit to that pod in your manifeast file but it is deployming if it consuming more then you will get this error]
  -> Liveness probe and readness probe [Livenss probe is used to check if your pod is in healthy state or not][readness probe is used to check if your pod is ready to receive the traffic or not]
  -> Keeping worng command line arguments [If you keep anything wrong in Dockerfile like filename]
  -> If you provide incorrect variables for container
  -> If your persistence volumes not existied

https://youtu.be/aEPIlQBWBGQ?si=LtQ6QpavKrWsYkAK



FailedScheduling Error:
  Based on below reasons we will get this error
  -> Node Selector [It will schedule the pod on particular Node]
  -> Node Affinity 
    In Node Affinity we have two options 
    # PreferredDuringSchedulingIgnoredDuringExecution (Preferred)
    -> This is a soft rule.
    -> The scheduler will try to place the Pod on nodes that match your affinity rules, but if none are available, it will fall back to any other node.
    -> Example use case: â€œI prefer running my Pod on SSD nodes, but it can run elsewhere if needed.â€

    # RequiredDuringSchedulingIgnoredDuringExecution (Required)
    This is a hard rule (just like NodeSelector, but more expressive).
    The Pod must be scheduled on a node that matches the rule.
    If no matching nodes exist, the Pod will stay Pending and never run until a suitable node is available.
    Example use case: â€œThis Pod must run on GPU nodes only.â€

# What is Taint
-> A taint is applied on a Node to repel Pods unless those Pods have a matching toleration.
-> Purpose: â€œDonâ€™t schedule Pods here unless they specifically tolerate this condition.â€
ðŸ‘‰ Use cases (your cluster upgrade example is correct):
-> When upgrading a node, you add a NoSchedule taint so that new Pods donâ€™t land on that node.
-> Existing Pods can be drained (kubectl drain) so that node becomes free for maintenance.
-> Also used for dedicated workloads (e.g., GPU nodes, system nodes).

  # Taint effects (3 types):
  -> NoSchedule â†’ No new Pods are scheduled unless they tolerate.
  -> NoExecute â†’ Evicts existing Pods (without toleration) and blocks new ones.
  -> PreferNoSchedule â†’ Tries to avoid placing Pods here, but may schedule if no better option.

# What is Toleration
-> A toleration is set in the Pod spec.
-> It allows the Pod to ignore the taint and still schedule on that Node.
-> Example: If a node has NoSchedule taint, but a Pod has the correct toleration, it can still be scheduled there.
ðŸ‘‰ Basically, taints = rules from Node, and tolerations = exceptions from Pod.

https://youtu.be/O61HDmGUBJM?si=l1O7IMkIqMPVFjDA

# Pod YAML with NodeSelector
apiVersion: v1
kind: Pod
metadata:
  name: node-selector-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
  nodeSelector:
    disktype: ssd

# Pod YAML with NodeAffinity
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-example
spec:
  containers:
  - name: nginx
    image: nginx:latest

  affinity:
    nodeAffinity:
      # --- Hard rule (Required) ---
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

      # --- Soft rule (Preferred) ---
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - us-east-1a

# Add a Taint to a Node
-> kubectl taint nodes worker-node1 key1=value1:NoSchedule

# Pod WITH Toleration (will schedule)
apiVersion: v1
kind: Pod
metadata:
  name: toleration-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"





-> File beat --> It will collect the logs
-> We have three container are running inside pod
-> First one we call it primay container
-> Second container we call it sidecar container. It will run as long as primary container running. where we install filebeat to get the logs
-> Third one we call it init container   --> It will start and complete the task before start primary container then it will exit or remove from the pod 


# Pod lifecycle state
-> Pending state --> Image still downloading or when suitable worker node not available then we will get these status
-> Running state  --> it will show if even single container is running inside our pod [suppose we have three container in our pod, even if single is running in our pod will show running state]
-> Successed -> Pod are sucessfully complete the task but container are exited with zero code 
-> failed  -> Pod are sucessfully executed the task but container exited with errors
-> crash loop backup error -> Inside the pod container start, crash, restart it is going like a loop
-> Terminated -> pod has forcely stop or deleted. Container inside the pods are shutdown
-> evicted --> Kubernetes itself delete the pods from worker node based on worker node resource


# Pod communicate
-> Containers in same pod it will communicate with localhost because it is sharing same network
-> pod to pod communication in same namespace, it will communicate using with service, it wont communicate with pod ips because if we restart the pod, pod ip will change. That's why we use service to communicate
-> pod to pod communication in different namespace, it will communicate using FQDN [Fully Qualified domain name]  throug this it will communicate [sevicename.namespace.svc.cluster.local]
-> pods in different worker nodes Communication works the same as same-node Pods Implemented via:
      CNI plugins (Calico, Flannel, Weave, Cilium)
-> pods in different cluster  
    -> vpe-peering
    -> Servicemesh
    -> API Gateway





